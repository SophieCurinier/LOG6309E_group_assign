{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "## Import needed librairies and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "data_df = pd.read_csv('result/HDFS/HDFS_ReadToExploitData.csv', engine='c', na_filter=False, memory_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the data into training and testing subsets\n",
    "We split the data into 70% training data and 30% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_data(x_data, y_data, train_ratio=0.5):\n",
    "    pos_idx = y_data > 0\n",
    "    x_pos = x_data[pos_idx]\n",
    "    y_pos = y_data[pos_idx]\n",
    "    x_neg = x_data[~pos_idx]\n",
    "    y_neg = y_data[~pos_idx]\n",
    "    train_pos = int(train_ratio * x_pos.shape[0])\n",
    "    train_neg = int(train_ratio * x_neg.shape[0])\n",
    "    x_train = np.hstack([x_pos[0:train_pos], x_neg[0:train_neg]])\n",
    "    y_train = np.hstack([y_pos[0:train_pos], y_neg[0:train_neg]])\n",
    "    x_test = np.hstack([x_pos[train_pos:], x_neg[train_neg:]])\n",
    "    y_test = np.hstack([y_pos[train_pos:], y_neg[train_neg:]])\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suffle and split the data into 70% training and 30% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-3242034010863403271</td>\n",
       "      <td>['09a53393', '3d91fa85', '09a53393', '09a53393...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_-7280517479581614900</td>\n",
       "      <td>['3d91fa85', '09a53393', '09a53393', '09a53393...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_1698958929618729205</td>\n",
       "      <td>['09a53393', '09a53393', '09a53393', '3d91fa85...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_5048469409603984999</td>\n",
       "      <td>['09a53393', '09a53393', '09a53393', '3d91fa85...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_-4442849309709851367</td>\n",
       "      <td>['09a53393', '09a53393', '09a53393', '3d91fa85...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId  \\\n",
       "0  blk_-3242034010863403271   \n",
       "1  blk_-7280517479581614900   \n",
       "2   blk_1698958929618729205   \n",
       "3   blk_5048469409603984999   \n",
       "4  blk_-4442849309709851367   \n",
       "\n",
       "                                       EventSequence  Label  \n",
       "0  ['09a53393', '3d91fa85', '09a53393', '09a53393...      0  \n",
       "1  ['3d91fa85', '09a53393', '09a53393', '09a53393...      0  \n",
       "2  ['09a53393', '09a53393', '09a53393', '3d91fa85...      0  \n",
       "3  ['09a53393', '09a53393', '09a53393', '3d91fa85...      0  \n",
       "4  ['09a53393', '09a53393', '09a53393', '3d91fa85...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "train_ratio = 0.7\n",
    "(x_train, y_train), (x_test, y_test) = _split_data(data_df['EventSequence'].values,\n",
    "    data_df['Label'].values, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 575061 instances, 16838 anomaly, 558223 normal\n",
      "Train: 402542 instances, 11786 anomaly, 390756 normal\n",
      "Test: 172519 instances, 5052 anomaly, 167467 normal\n",
      "\n",
      "====== x_train (first five lines) ======\n",
      "[\"['09a53393', '09a53393', '09a53393', '3d91fa85', 'd38aa58d', 'e3df2680', 'd38aa58d', 'e3df2680', 'd38aa58d', 'e3df2680', '5d5de21c', '5d5de21c', '5d5de21c', 'd63ef163', 'd63ef163', 'd63ef163', 'dba996ef', 'dba996ef', '2e68ccc3', 'dba996ef']\"\n",
      " \"['3d91fa85', '09a53393', '09a53393', '09a53393', 'd38aa58d', 'e3df2680', 'd38aa58d', 'e3df2680', '5d5de21c', 'e3df2680', '5d5de21c', '5d5de21c', 'd38aa58d', '81cee340', '626085d5', '81cee340', '32777b38', 'd63ef163', 'd63ef163', 'd63ef163', 'dba996ef', 'dba996ef', '8f2bc724', '5d5de21c', 'dba996ef']\"\n",
      " \"['09a53393', '3d91fa85', '09a53393', 'bcc910df']\"\n",
      " \"['09a53393', '09a53393', '3d91fa85', 'bcc910df']\"\n",
      " \"['09a53393', '3d91fa85', '09a53393', 'bcc910df']\"]\n",
      "====== y_train (first five lines) ======\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "num_train = x_train.shape[0]\n",
    "num_test = x_test.shape[0]\n",
    "num_total = num_train + num_test\n",
    "num_train_pos = sum(y_train)\n",
    "num_test_pos = sum(y_test)\n",
    "num_pos = num_train_pos + num_test_pos\n",
    "\n",
    "print('Total: {} instances, {} anomaly, {} normal' \\\n",
    "      .format(num_total, num_pos, num_total - num_pos))\n",
    "print('Train: {} instances, {} anomaly, {} normal' \\\n",
    "      .format(num_train, num_train_pos, num_train - num_train_pos))\n",
    "print('Test: {} instances, {} anomaly, {} normal\\n' \\\n",
    "      .format(num_test, num_test_pos, num_test - num_test_pos))\n",
    "\n",
    "#print(type(x_train))\n",
    "print('====== x_train (first five lines) ======')\n",
    "print(x_train[:5])\n",
    "\n",
    "print('====== y_train (first five lines) ======')\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **transform_train_data** function takes a collection of log sequences X_seq as input. It counts the occurrences of each event in each log sequence and stores these counts in a structured format. It then returns two things: a feature matrix X where rows represent log sequences and columns represent event counts, and a list of event names events. This transformation allows the log data to be used in machine learning models for anomaly detection.\n",
    "\n",
    "Row = block_id\n",
    "Colum = Event Type\n",
    "x_train[i][j] = nb of event #j in block_id #i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction : Message Count Matrix = Message Count Vector for each block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Transformed train data summary ======\n",
      "Train data shape: 402542-by-21\n",
      "\n",
      "[[ 1. 40.  6. 13. 13. 13. 23. 19. 19. 22.  7. 10. 11. 13.  7. 13.  6.  3.\n",
      "   1.  0.  0.]\n",
      " [ 1. 50.  9. 13. 13. 17. 26. 24. 24. 25. 10. 11. 15. 17. 11. 14.  7.  5.\n",
      "   1.  3.  4.]\n",
      " [ 1.  8.  3.  6.  3.  3.  7.  3.  3.  2.  2.  2.  1.  0.  0.  0.  2.  1.\n",
      "   1.  0.  0.]\n",
      " [ 1.  8.  3.  6.  3.  3.  7.  3.  3.  2.  2.  2.  1.  0.  0.  0.  2.  1.\n",
      "   1.  0.  0.]\n",
      " [ 1.  8.  3.  6.  3.  3.  7.  3.  3.  2.  2.  2.  1.  0.  0.  0.  2.  1.\n",
      "   1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def transform_train_data(X_seq):\n",
    "    X_counts = []\n",
    "    for i in range(X_seq.shape[0]):\n",
    "        event_counts = Counter(X_seq[i])\n",
    "        X_counts.append(event_counts)\n",
    "    X_df = pd.DataFrame(X_counts)\n",
    "    X_df = X_df.fillna(0)\n",
    "    events = X_df.columns\n",
    "    X = X_df.values\n",
    "    return (X, events)\n",
    "\n",
    "transformed = transform_train_data(x_train)\n",
    "x_train_mcv = transformed[0]\n",
    "# The events in the training data,\n",
    "# used later to ingore the events in the test data that is unseen in the training data\n",
    "events_mcv = transformed[1]\n",
    "\n",
    "print('====== Transformed train data summary ======')\n",
    "print('Train data shape: {}-by-{}\\n'.format(x_train_mcv.shape[0], x_train_mcv.shape[1]))\n",
    "print(x_train_mcv[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Transformed test data summary ======\n",
      "Test data shape: 172519-by-21\n",
      "\n",
      "[[ 1. 58. 12. 18. 16. 20. 29. 28. 28. 27. 11. 11. 14. 19. 10. 19.  9.  6.\n",
      "   1.  5.  6.]\n",
      " [ 1.  4.  1.  3.  2.  2.  4.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.]\n",
      " [ 1. 54.  9. 18. 16. 18. 29. 26. 26. 27. 10. 12. 11. 16.  9. 19.  7.  6.\n",
      "   1.  3.  6.]\n",
      " [ 1.  4.  1.  3.  2.  2.  4.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.]\n",
      " [ 1. 40.  7. 14. 13. 13. 22. 19. 19. 23.  7. 11. 10. 12.  8. 12.  4.  3.\n",
      "   1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def transform_test_data(X_seq, events):\n",
    "    X_counts = []\n",
    "    for i in range(X_seq.shape[0]):\n",
    "        event_counts = Counter(X_seq[i])\n",
    "        X_counts.append(event_counts)\n",
    "    X_df = pd.DataFrame(X_counts)\n",
    "    X_df = X_df.fillna(0)\n",
    "    # treat the counts of the missing events as 0s\n",
    "    empty_events = set(events) - set(X_df.columns)\n",
    "    for event in empty_events:\n",
    "        X_df[event] = [0] * len(X_df)\n",
    "    X = X_df[events].values\n",
    "    return X\n",
    "\n",
    "x_test_mcv = transform_test_data(x_test, events_mcv)\n",
    "\n",
    "print('====== Transformed test data summary ======')\n",
    "print('Test data shape: {}-by-{}\\n'.format(x_test_mcv.shape[0], x_test_mcv.shape[1]))\n",
    "print(x_test_mcv[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction : TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Transformed test data summary -- TF-IDF ======\n",
      "Test data shape: 402542-by-48\n",
      "\n",
      "   0567184d  06d16156  09a53393  0f86472a  124068c6  13eb7010  234302e6  \\\n",
      "0       0.0       0.0  0.305852       0.0       0.0       0.0       0.0   \n",
      "1       0.0       0.0  0.236314       0.0       0.0       0.0       0.0   \n",
      "2       0.0       0.0  0.305223       0.0       0.0       0.0       0.0   \n",
      "3       0.0       0.0  0.305223       0.0       0.0       0.0       0.0   \n",
      "4       0.0       0.0  0.305223       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   2e68ccc3  2ecc047e  2f85639c  ...  d63ef163  d6b7b743  dba996ef  e024fa48  \\\n",
      "0  0.578839       0.0       0.0  ...  0.370805       0.0  0.369753       0.0   \n",
      "1  0.000000       0.0       0.0  ...  0.286500       0.0  0.285688       0.0   \n",
      "2  0.000000       0.0       0.0  ...  0.000000       0.0  0.000000       0.0   \n",
      "3  0.000000       0.0       0.0  ...  0.000000       0.0  0.000000       0.0   \n",
      "4  0.000000       0.0       0.0  ...  0.000000       0.0  0.000000       0.0   \n",
      "\n",
      "   e3df2680  f0d1ff15  f266840a  f79898ae  fcd37a6d  ff00cd08  \n",
      "0  0.309192       0.0       0.0       0.0       0.0       0.0  \n",
      "1  0.238896       0.0       0.0       0.0       0.0       0.0  \n",
      "2  0.000000       0.0       0.0       0.0       0.0       0.0  \n",
      "3  0.000000       0.0       0.0       0.0       0.0       0.0  \n",
      "4  0.000000       0.0       0.0       0.0       0.0       0.0  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "def transform_train_data_tfidf(X_seq):\n",
    "    tokenized_sequences = [eval(x) for x in X_seq]\n",
    "\n",
    "    # Join the tokens back into space-separated strings (required for TF-IDF)\n",
    "    document_strings = [' '.join(x) for x in tokenized_sequences]\n",
    "\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the event sequences into TF-IDF vectors\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(document_strings)\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for further analysis if needed\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "# Assuming x_train contains your training data\n",
    "x_train_tf_idf = transform_train_data_tfidf(x_train)\n",
    "\n",
    "print('====== Transformed test data summary -- TF-IDF ======')\n",
    "print('Test data shape: {}-by-{}\\n'.format(x_train_tf_idf.shape[0], x_train_tf_idf.shape[1]))\n",
    "print(x_train_tf_idf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Transformed test data summary -- TF-IDF ======\n",
      "Test data shape: 172519-by-48\n",
      "\n",
      "   0567184d  06d16156  09a53393  0f86472a  124068c6  13eb7010  234302e6  \\\n",
      "0       0.0       0.0  0.253114       0.0       0.0       0.0       0.0   \n",
      "1       0.0       0.0  0.707107       0.0       0.0       0.0       0.0   \n",
      "2       0.0       0.0  0.264222       0.0       0.0       0.0       0.0   \n",
      "3       0.0       0.0  0.707107       0.0       0.0       0.0       0.0   \n",
      "4       0.0       0.0  0.275701       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   2e68ccc3  2ecc047e  2f85639c  ...  d63ef163  d6b7b743  dba996ef  e024fa48  \\\n",
      "0       0.0       0.0       0.0  ...  0.230150  0.361142  0.305997       0.0   \n",
      "1       0.0       0.0       0.0  ...  0.000000  0.000000  0.000000       0.0   \n",
      "2       0.0       0.0       0.0  ...  0.320335  0.376992  0.319426       0.0   \n",
      "3       0.0       0.0       0.0  ...  0.000000  0.000000  0.000000       0.0   \n",
      "4       0.0       0.0       0.0  ...  0.334252  0.000000  0.333304       0.0   \n",
      "\n",
      "   e3df2680  f0d1ff15  f266840a  f79898ae  fcd37a6d  ff00cd08  \n",
      "0  0.191909       0.0       0.0       0.0       0.0       0.0  \n",
      "1  0.000000       0.0       0.0       0.0       0.0       0.0  \n",
      "2  0.200331       0.0       0.0       0.0       0.0       0.0  \n",
      "3  0.000000       0.0       0.0       0.0       0.0       0.0  \n",
      "4  0.278713       0.0       0.0       0.0       0.0       0.0  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "def transform_test_data_tfidf(X_seq, tfidf_vectorizer):\n",
    "    tokenized_sequences = [eval(x) for x in X_seq]\n",
    "\n",
    "    # Join the tokens back into space-separated strings (required for TF-IDF)\n",
    "    document_strings = [' '.join(x) for x in tokenized_sequences]\n",
    "\n",
    "    # Transform the test data using the same TF-IDF vectorizer as used for training\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(document_strings)\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for further analysis if needed\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "# Assuming x_test contains your test data\n",
    "x_test_tf_idf = transform_test_data_tfidf(x_test, tfidf_vectorizer)\n",
    "\n",
    "print('====== Transformed test data summary -- TF-IDF ======')\n",
    "print('Test data shape: {}-by-{}\\n'.format(x_test_tf_idf.shape[0], x_test_tf_idf.shape[1]))\n",
    "print(x_test_tf_idf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_mcv\n",
    "x_test = x_test_mcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classique Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lr_classifier = LogisticRegression(penalty='l2', C=100, tol=0.01, class_weight=None, max_iter=1000)\n",
    "\n",
    "# lr_classifier.fit(x_train_mcv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance of the logistic regression model\n",
    "\n",
    "We evaluate the model's performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Test validation:')\n",
    "# print('====== Evaluation summary ======')\n",
    "# y_test_pred_lr = lr_classifier.predict(x_test_mcv)\n",
    "\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred_lr, average='binary')\n",
    "# print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_pred_proba_lr = lr_classifier.predict_proba(x_test_mcv)[:,1] # predicted probabilities for being \"anomaly\"\n",
    "\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred_proba_lr)\n",
    "\n",
    "# plt.step(recall, precision, color='b', alpha=0.2,\n",
    "#          where='post')\n",
    "# plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "#                  color='b')\n",
    "\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('2-class Precision-Recall curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_pred_proba_lr = lr_classifier.predict_proba(x_test_mcv)[:,1] # predicted probabilities for being \"anomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_roc_curve(fpr, tpr):\n",
    "#     plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "#     plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba_lr)\n",
    "# plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc = roc_auc_score(y_test, y_test_pred_proba_lr)\n",
    "# print('AUC: %.3f\\n' % roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# svm_classifier = SVC(kernel='linear', C=100, random_state=42, max_iter=1000, class_weight=None, penalty=12)\n",
    "\n",
    "# # Fit the SVM model on the training data\n",
    "# svm_classifier.fit(x_train_mcv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Test validation:')\n",
    "# print('====== Evaluation summary ======')\n",
    "# y_pred = svm_classifier.predict(x_test_mcv)\n",
    "\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred_lr, average='binary')\n",
    "# print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_classifier = DecisionTreeClassifier(criterion='gini', max_depth=None,\n",
    "#                           max_features=None, class_weight=None)\n",
    "\n",
    "# # Fit the Decision Tree model on the training data\n",
    "# # dt_classifier.fit(x_train_mcv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Test validation:')\n",
    "# print('====== Evaluation summary ======')\n",
    "# y_test_pred_dt = dt_classifier.predict(x_test_mcv)\n",
    "\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred_dt, average='binary')\n",
    "# print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', C=1.0, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(criterion='gini', max_depth=None,\n",
    "                          max_features=None, class_weight=None),\n",
    "    'LogisticRegression': LogisticRegression(penalty='l2', C=100, tol=0.01, class_weight=None, max_iter=1000)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over models\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n",
    "    \n",
    "    # Append results to the list\n",
    "    results.append({'Model': model_name, 'Accuracy': accuracy, 'F1 Score': f1, 'Precision': precision, 'AUC': auc})\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('result/HDFS/model_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
